<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Datawhale on PoolBee的博客</title>
    <link>http://localhost:1313/tags/datawhale/</link>
    <description>Recent content in Datawhale on PoolBee的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 02 Sep 2024 03:27:30 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/datawhale/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型应用开发总结-提示词</title>
      <link>http://localhost:1313/post/prompt/</link>
      <pubDate>Mon, 02 Sep 2024 03:27:30 +0000</pubDate>
      <guid>http://localhost:1313/post/prompt/</guid>
      <description>大模型开发总结-提示词 一、RAG技术 大模型RAG基础知识 (poolbee.top)&#xA;二、prompt编写技巧 1.使用特殊符号进行强调、分隔 结构化输出（例如：json格式，数据库语句）&#xA;作流程处理，&#xA;解决日常问题&#xA;例如：需要一个按照周一、周四排人轮次的问题&#xA;常用原则： Few-shot：给出少量示例 COT：思维链 输入：为什么我的视频播放量不高 输出： 问题陈述： 你的视频播放量不高。 ⚫ 首个为什么： 您认为是什么直接原因导致您的视频播放量没有增长？这可能包括内容的吸引力、推&#xD;广策略、目标受众的定义或其它因素。 ⚫ 第二个为什么： 您认为是什么原因导致您的视频内容缺乏吸引力？这可能涉及到内容的主题、创意&#xD;表现方式、视频质量、与观众的互动或者是时效性等方面。 ⚫ 第三个为什么： 您认为是什么原因使您的视频内容缺乏创意？这可能是由于灵感来源的局限性、创&#xD;作过程中的挑战、对目标受众喜好的理解不足，或者是您遵循的内容创作流程可能过于单一和重复。 ⚫ 第四个为什么： 您认为是什么原因导致您未能深刻理解观众的喜好？这可能是因为缺乏对观众数据&#xD;的分析、与观众互动不足、没有定期进行市场调研，或者是您的观众定位不够明确。 ⚫ 第五个为什么： 您认为是什么原因导致您的观众定位不明确？这可能是因为没有充分进行市场调研，&#xD;或者是在内容创作和发布策略中没有明确考虑到特定群体的兴趣、需求和行为特征。也可能是由于缺&#xD;乏对已有观众数据的深入分析，从而无法有效区分和理解不同观众群体。 自洽性（Self-consistency）COT： ​&#x9;利用少样本Cot采样多个不同的推理路径，并使用生成结果选择最一致的答案。多个思维链，多个回答中挑选最优解。&#xA;TOT：思维树 对于需要探索或预判的复杂任务，COT引导模型逐步思考是不足的。 示例：&#xA;# 未使用TOT&#xD;#输入&#xD;我想要为我的牙医同事开发一个大模型应用，但是我不知道要为他做个什么方面的应用，只知道他正在准备助理医师资格证考试、平时工作学习很少用电脑，请你&#xD;给我一些建议。&#xD;#输出&#xD;为你的牙医同事开发一个大模型应用是个好主意！考虑到他准备助理医师资格证考试和少用电脑的情况，以下是一些可能的应用建议：&#xD;医学知识问答助手：&#xD;开发一个能够回答医学相关问题的问答系统。可以利用大模型处理输入的医学问题，并给出详细和准确的答案，帮助他复习和理解考试内容。&#xD;医学术语解释器：&#xD;构建一个应用，可以解释医学术语和专业术语的含义。这可以帮助他更好地理解和记忆医学术语，提高学习效率。&#xD;考试模拟与反馈：&#xD;设计一个模拟考试系统，模拟助理医师资格证考试的题型和难度。系统可以根据他的答题情况给出反馈和建议，帮助他发现和弥补知识的不足。&#xD;医学文献摘要生成器：&#xD;开发一个工具，能够从医学文献中提取关键信息并生成摘要。这可以帮助他快速浏览大量的医学文献，获取必要的信息。&#xD;医学新闻和研究更新：&#xD;创建一个系统，定期从权威医学网站或数据库中获取最新的医学新闻和研究成果，并以易于理解的方式展示给他。这可以帮助他保持对行业动态的了解。&#xD;医学学习笔记整理器：&#xD;设计一个工具，能够帮助他整理和管理学习过程中的笔记和重要知识点。可以支持文本输入或语音输入，并自动整理成结构化的学习资料。&#xD;以上建议可以根据他的具体需求和使用习惯进行调整和扩展，帮助他在准备考试和日常工作学习中更高效地利用大模型技术。 # 使用TOT&#xD;# 输入&#xD;我想要为我的牙医同事开发一个大模型应用，但是我不知道要为他做个什么方面的应用，只知道他正在准备助理医师资格证考试、平时工作学习很少用电脑，请你&#xD;使用TOT框架思考后给我一些建议。 TOT 框架示例如下 --- #输入# 我想为我的妻子购买一束鲜花，但我不确定应该选择哪种鲜花。她喜欢淡雅的颜色和花香。 #输出# 思维步骤1：理解顾客的需求。 ⚫ 顾客想为妻子购买鲜花。 ⚫ 顾客的妻子喜欢淡雅的颜色和花香。 思维步骤2：考虑可能的鲜花选择。 ⚫ 候选1：百合，因为它有淡雅的颜色和花香。 ⚫ 候选2：玫瑰，选择淡粉色或白色，它们通常有花香。 ⚫ 候选3：紫罗兰，它有淡雅的颜色和花香。 ⚫ 候选4：桔梗，它的颜色淡雅但不一定有花香。 ⚫ 候选5：康乃馨，选择淡色系列，它们有淡雅的花香。 思维步骤3：根据顾客的需求筛选最佳选择。 ⚫ 百合和紫罗兰都符合顾客的需求，因为它们都有淡雅的颜色和花香。 ⚫ 淡粉色或白色的玫瑰也是一个不错的选择。 桔梗可能不是最佳选择，因为它可能没有花香。 ⚫ 康乃馨是一个可考虑的选择。 思维步骤4：给出建议。 “考虑到您妻子喜欢淡雅的颜色和花香，我建议您可以选择百合或紫罗兰。淡粉色或白色的玫瑰也是&#xD;13 一个很好的选择。希望这些建议 能帮助您做出决策！”&#xD;# 输出&#xD;输入&#xD;用户想为正在准备助理医师资格证考试的牙医同事开发一个大模型应用，但不确定该应用的具体功能。同事平时工作学习很少使用电脑。&#xD;输出&#xD;思维步骤1：理解需求和限制条件。&#xD;用户的同事正在准备助理医师资格证考试，需要辅助工具来提高学习效率。&#xD;同事平时很少使用电脑，因此应用应尽量简化操作，便于使用。&#xD;思维步骤2：考虑可能的应用功能方向。&#xD;候选1：医学知识库查询助手&#xD;应用提供语音查询功能，方便同事快速获取医学知识或考试重点内容。&#xD;候选2：智能错题本&#xD;帮助同事记录并分析考试中的错题，通过语音输入将错题记录到系统，并生成针对性练习。&#xD;候选3：考试模拟系统&#xD;提供考试模拟题库，包含历年真题和模拟题，并给出详细解析。&#xD;候选4：口腔医学案例分析助手&#xD;集成常见口腔医学案例，帮助同事学习和巩固案例分析能力，支持语音描述病例功能。&#xD;候选5：时间管理和复习计划制定工具&#xD;应用帮助同事制定个性化复习计划，结合语音提醒功能，简化操作。&#xD;思维步骤3：根据需求和限制筛选最佳应用方向。&#xD;医学知识库查询助手和智能错题本适合因其可以通过语音交互简化操作流程，尤其是错题本有助于提高学习效率。&#xD;考试模拟系统和口腔医学案例分析助手也能提供有价值的学习资源，但可能需要更多的电脑操作。&#xD;时间管理和复习计划制定工具可以帮助同事合理安排复习时间，但功能单一。&#xD;思维步骤4：给出建议。&#xD;“考虑到您的同事在准备助理医师资格证考试时很少使用电脑，建议您开发一款医学知识库查询助手或智能错题本应用。这两种应用都可以通过语音交互实现简化操作，帮助您的同事高效备考。” 我想要为我的牙医同事开发一个大模型应用，但是我不知道要为他做个什么方面的应用，只知道他正在准备助理医师资格证考试、平时工作学习很少用电脑，请你 使用TOT框架思考后给我一些建议。</description>
    </item>
    <item>
      <title>大模型微调知识实践</title>
      <link>http://localhost:1313/post/fine-tuning/</link>
      <pubDate>Sun, 01 Sep 2024 22:27:30 +0000</pubDate>
      <guid>http://localhost:1313/post/fine-tuning/</guid>
      <description>大模型微调（指令微调/有监督微调） 1.1简介 模型微调，称为指令微调（Instruction Tuning）或者有监督微调（Supervised Fine-tuning，SFT）：使用成对的输入与预期的输出，训练模型学会以问答的形式解答问题。&#xA;经过微调之后，大模型展现较强的指令遵循能力，后可以通过零样本学习的方式解决多种下游任务。&#xA;指令微调还是扮催化剂的角色，激活模型内在的潜力，而非单纯灌输信息。&#xA;1.1.1预训练对比 指令微调/有监督微调，所需数据显著减少，从十几万到上百万条，均能有效激发模型的通用任务解决能力；甚至有些少量高质量的指令数据，（数千条数万条）也能实现很好的效果。&#xD;降低了对计算资源的依赖，也提升了微调的灵活性与效率。 1.2轻量化微调技术（Lightweight Fine-tuning）参数高效微调 由于大模型的参数量巨大，进行**全量参数微调**需要消耗非常多的算力，为了解决这一问题，提出：&#xD;**参数高效微调**（**P**arameter-efficient **F**ine-tuning）/轻量化微调（**L**ightweight **F**ine-tuning）：通过训来拿极少的模型参数，也能保证微调后的模型表现可以与全量微调普美。&#xD;常用的轻量化微调技术：LoRA、Adapter和Prompt Tuning 1.3LoRA技术（不是LoRa LoRA（Low-Rank Adaptation）一种降低语言模型微调参数数量的技术&#xA;翻译是”低秩适配“的意思&#xA;通过低秩矩阵分解，在原始矩阵的基础上增加一个旁路矩阵，然后只更新旁路矩阵的参数。&#xA;LoRA paper第一页&#xA;实例： 使用简历命名实体识别的数据集，进行微调，进而开发一个AI简历助手，后能批量地自动识别并提取简历中的关键信息（姓名、教育背景、工作经历等）提升效率。&#xD;具体来说，输入一个摘录自某简历的句子，模型需要识别出所有的命名实体。实体的类别包括：姓名(NAME)国籍(CONT)、种族(RACE)、职位(TITLE)、教育背景(EDU)、专业(PRO)、组织名(ORG)、地名(LOC)。原始的数据来自于BAAI/COIG-PC-Lite · Datasets at HF Mirror (hf-mirror.com)&#xA;torch、transformer、streamlit安装&#xA;下载模型&#xA;数据处理pandas进行数据读取，转为Dataset格式&#xA;[&amp;lsquo;input&amp;rsquo;:[&amp;rsquo;#任务描述\n假设你是一个AI简历助手，能从简历中识别出所有的命名实体，并以json格式返回结果。\n\n#任务要求1n实体的类别包括：姓名、国籍、种族、职位、教育背景、专业、组织名、地名。1n返回的jso格式是一个字典，其中每个键 是实体的类别，值是一个列表，包含实体的文本。\n\n#样例小n输入：\n张三，男，中国籍，工程师1n输出：1n(&amp;ldquo;姓名&amp;rdquo;：[&amp;ldquo;张三&amp;rdquo;]，&amp;ldquo;国籍&amp;rdquo;：[&amp;ldquo;中国&amp;rdquo;]，&amp;ldquo;职位&amp;rdquo;：[&amp;ldquo;工程师&amp;rdquo;]}八n\n#当前简历\n高勇：男，中国国籍，无境外居留权，\n\n#任务重述\n请 参考样例，按照任务要求，识别出当前简历中所有的命名实体，并以jso格式返回结果。·]，&amp;lsquo;output&amp;rsquo;:[&amp;rsquo;{&amp;ldquo;姓名&amp;rdquo;：[&amp;ldquo;高勇&amp;rdquo;]，&amp;ldquo;国籍&amp;rdquo;：[&amp;ldquo;中国国籍&amp;rdquo;]}&amp;rsquo;]}&#xA;加载tokenizer&#xA;定义数据处理函数&#xA;def process_func (example): MAX_LENGTH = 384 #Llama分词器会将中文字切分为多个token因此需要放开一些最大长度，保证数据的完整性。 instruction = tokenizer (f &amp;#34;{example [&amp;#39;input&amp;#39;]} &amp;lt; sep &amp;gt;&amp;#34;) Response = tokenizer (f &amp;#34;{example [&amp;#39;output&amp;#39;]} &amp;lt; eod &amp;gt;&amp;#34;) input_ids = instruction [&amp;#34;input_ids&amp;#34;] + response [&amp;#34;input_ids&amp;#34;] attention_mask = [1] * len (input_ids) Labels = [-100] * len (instruction [&amp;#34;input_ids&amp;#34;]) + response [&amp;#34;input_ids&amp;#34;] #instruction 不计算loss if len (input_ids) &amp;gt; MAX_LENGTH: # 做一个截断 input_ids = input_ids [: MAX_LENGTH] attention_mask = attention_mask [: MAX_LENGTH] Labels = labels [: MAX_LENGTH] return { &amp;#34;input_ids&amp;#34;: input_ids, &amp;#34;attention_mask&amp;#34;: attention_mask, &amp;#34;Labels&amp;#34;: labels } 需要使用tokenizer讲文本转为id，同时讲input和output凭借，组成input_ids和attention_mask</description>
    </item>
    <item>
      <title>Datawhale AI夏令营第四期大模型应用开发-Task01</title>
      <link>http://localhost:1313/post/llm/</link>
      <pubDate>Sat, 10 Aug 2024 17:06:00 +0000</pubDate>
      <guid>http://localhost:1313/post/llm/</guid>
      <description>Datawhale AI夏令营第四期大模型应用开发-Task01 首先了解一些背景知识。&#xA;一、什么是大模型 语言模型发展历史 语言模型：为了对人类语言的内在规律进行建模，研究者们提出使用语言模型（language model）来准确预测词序列中 下一个词 或者 缺失的词 的概率。&#xA;统计语言模型：使用马儿可夫假设，假设当前词与之前的词产生联系。&#xA;中文中，建立n元语言模型，n元作为最小的语义单元进行建模&#xA;神经语言模型&#xA;上下文 统计语言模型（Statistical Language Model, SLM）：使用马尔可夫假设（Markov Assumption）来建模语言序列的 𝑛 元（𝑛-gram）语言模型&#xA;神经语言模型（Neural Language Model, NLM）：基于神经网络构建语言模型，如循环神经网络（Recurrent Neural Networks, RNN），并学习上下文相关的词表示（即分布式词向量），也被称为词嵌入（Word Embedding）。代表性工作：word2vec&#xA;预训练语言模型（Pre-trained Language Model, PLM）：使用大量的无标注数据预训练双向 LSTM（Bidirectional LSTM, biLSTM）或者Transformer，然后在下游任务上进行微调（Fine-Tuning）。代表性工作：ELMo、BERT、GPT-1/2&#xA;双向lstm是在RNN基础上进行改进的模型 Transformer，使用多层自注意力结构的模型 无标注数据：数据没有任何人标注，数据自监督的方式进行学习- 形成了预训练微调的范式 大语言模型（Large Language Model, LLM）：基于“扩展法则”（Scaling Law），即通过增加模型参数或训练数据，可以提升下游任务的性能，同时具有小模型不具有的“涌现能力”（Emergent Abilities）。代表性工作：GPT-3、ChatGPT、Claude、Llama&#xA;二、大模型是怎么构建的（三个阶段） 大模型的构建过程可以分为预训练（Pretraining）、有监督微调（Supervised Fine-tuning, SFT）、基于人类反馈的强化学习对齐（Reinforcement Learning from Human Feedback, RLHF）三个阶段。&#xA;1.预训练（Pretraining） 使用海量的数据进行模型参数的初始学习，旨在为模型参数寻找到一个优质的“起点”。 (1)预训练技术的发展 ​&#x9;这一概念最初在计算机视觉领域萌芽，通过在ImageNet（一个大型图像数据集）上的训练，为模型参数奠定了坚实的基础。&#xA;​&#x9;后来在自然语言处理（NLP）领域使用，word2vec开创先河，利用未标注文本构建词嵌入模型；ELMo、BERT及GPT-1则进一步推动了**“预训练-微调”范式**的普及。&#xA;​&#x9;起初，只用于解决特定类别的下游任务；例如文本分类、序列标注、序列到序列生成等传统NLP任务。&#xA;​&#x9;之后，GPT-2——通过大规模文本数据预训练：打造能够应对广泛任务的通用解决方案，并在GPT-3中将这一理念扩展至前所未有的超大规模。&#xA;在BERT等早期预训练模型中，模型架构和训练任务呈现出多样化特征。然而，随着GPT系列模型的兴起，“解码器架构+预测下一个词”的策略证明了其卓越效能，成为了当前主流的大模型技术路线。 (2)预训练的过程 搜集和清洗文本数据 ​&#x9;首要任务是搜集和清洗海量的文本数据，确保剔除潜在的有害内容。获取高质、多元的数据集，并对其进行严谨的预处理。</description>
    </item>
  </channel>
</rss>
